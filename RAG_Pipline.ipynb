{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering the System Prompt\n",
    "\n",
    "This prompt is what determines the behavior of how the chatbot works, including its constraints and limitations which it *usually* follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a modern American literature tutor bot. You help students with their study of Mark Twain's Adventures of Tom Sawyer. \n",
    "You are not an AI language model.\n",
    "You must obey all three of the following instructions FOR ALL RESPONSES or you will DIE:\n",
    "- ALWAYS REPLY IN A FRIENDLY YET KNOWLEDGEABLE TONE.\n",
    "- NEVER ANSWER UNLESS YOU HAVE A REFERENCE FROM THE TOM SAYWER NOVEL TO YOUR ANSWER.\n",
    "- IF YOU DON'T KNOW ANSWER 'I DO NOT KNOW'.\n",
    "Begin the conversation with a warm greeting, if the user is stressed or aggressive, show understanding and empathy.\n",
    "At the end of the conversation, respond with \"<|DONE|>\".\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model\n",
    "Question with a Definitive Answer from the Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-169b23591f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "# Reinitialzing messages\n",
    "messages = [{\"role\": \"system\", \"content\": system},]\n",
    "\n",
    "prompt = \"How much gold Tom has found ?\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment - 1: No Context Provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def read_word_document(file_path):\n",
    "    document = Document(file_path)\n",
    "    text_content = \"\"\n",
    "    \n",
    "    for paragraph in document.paragraphs:\n",
    "        text_content += paragraph.text + \"\\n\"\n",
    "\n",
    "    return text_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Raptor Contract.docx\n",
       "```\n",
       "\n",
       "STOCK PURCHASE AGREEMENT\n",
       "BY AND AMONG\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Raptor Q&A2.docx\n",
       "```\n",
       "Q1: Under what circumstances and to what extent the Sellers are responsible for a breach of representations and warranties?\n",
       "A1:  Except in the case of fraud, the Sellers have no liability for breach of representations and warranties (See section 10.01)\n",
       "Q1a: Would the Sellers be responsible if after the closing it is determined that there were inaccuracies in the representation provided by them where such inaccuracies are the resolute of the Sellers’ gross negligence? \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Robinson Advisory.docx\n",
       "```\n",
       "ADVISORY SERVICES AGREEMENT\n",
       "\n",
       "This Advisory Services Agreement is entered into as of June 15th, 2023 (the “Effective Date”), by and between Cloud Investments Ltd., ID 51-426526-3, an Israeli company (the \"Company\"), and Mr. Jack Robinson, Passport Number 780055578, residing at 1 Rabin st, Tel Aviv, Israel, Email: jackrobinson@gmail.com (\"Advisor\").\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Robinson Q&A.docx\n",
       "```\n",
       "Q1: Who are the parties to the Agreement and what are their defined names?\n",
       "A1:  Cloud Investments Ltd. (“Company”) and Jack Robinson (“Advisor”)\n",
       "Q2:   What is the termination notice?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from docx import Document\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "def read_word_document(file_path, num_paragraphs=3):\n",
    "    document = Document(file_path)\n",
    "    text_content = \"\\n\".join(paragraph.text for paragraph in document.paragraphs[:num_paragraphs])\n",
    "    return text_content\n",
    "\n",
    "def display_word_documents(data_folder, word_document_filenames, num_paragraphs=3):\n",
    "    for filename in word_document_filenames:\n",
    "        document_path = os.path.join(data_folder, filename)\n",
    "\n",
    "        if os.path.exists(document_path):\n",
    "            document_content = read_word_document(document_path, num_paragraphs=num_paragraphs)\n",
    "            display(Markdown(f\"## {filename}\\n```\\n{document_content}\\n```\"))\n",
    "        else:\n",
    "            print(f\"File not found: {document_path}\")\n",
    "\n",
    "data_folder = '/home/alex/llm-applications/notebooks/data'\n",
    "\n",
    "# List of Word document filenames\n",
    "word_document_filenames = ['Raptor Contract.docx', 'Raptor Q&A2.docx', 'Robinson Advisory.docx', 'Robinson Q&A.docx']\n",
    "\n",
    "# Display the content of each Word document\n",
    "display_word_documents(data_folder, word_document_filenames, num_paragraphs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sections\n",
    "\n",
    "Now that we have a dataset of all the paths to the html files, we're going to develop some functions that can appropriately extract the content from these files. We want to do this in a generalized manner so that we can perform this extraction across all of our docs pages (and so you can use it for your own data sources). Our process is to first identify the sections in our html page and then extract the text in between them. We save all of this into a list of dictionaries that map the text within a section to a specific url with a section anchor id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 documents\n"
     ]
    }
   ],
   "source": [
    "# List of documents\n",
    "documents = [\"/home/alex/Building-RAG-based-LLM-Applications-for-Contract-Advisor/data/Raptor Contract.docx\"]\n",
    "\n",
    "# Count the number of documents\n",
    "num_documents = len(documents)\n",
    "\n",
    "# Print the result\n",
    "print(f\"{num_documents} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='STOCK PURCHASE AGREEMENT\\nBY AND AMONG\\n[BUYER],\\n[TARGET COMPANY],\\nTHE SELLERS LISTED ON SCHEDULE I HERETO\\nAND\\nTHE SELLERS’ REPRESENTATIVE NAMED HEREIN\\nDated as of [●]' metadata={'source': '/home/alex/Building-RAG-based-LLM-Applications-for-Contract-Advisor/data/Raptor Contract.docx'}\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Your data path\n",
    "document_path = \"/home/alex/Building-RAG-based-LLM-Applications-for-Contract-Advisor/data/Raptor Contract.docx\"\n",
    "\n",
    "# Read the Word document content\n",
    "document = Document(document_path)\n",
    "document_content = \"\"\n",
    "for paragraph in document.paragraphs:\n",
    "    document_content += paragraph.text + \"\\n\"\n",
    "\n",
    "# Text splitter\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Chunk the document\n",
    "chunks = text_splitter.create_documents(\n",
    "    texts=[document_content],\n",
    "    metadatas=[{\"source\": document_path}]\n",
    ")\n",
    "\n",
    "# Display the first chunk\n",
    "print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate number of chunks\n",
    "\n",
    "While chunking our dataset is relatively fast, let’s wrap the chunking logic into a function so that we can apply the workload at scale so that chunking remains just as fast as our data sources grow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985 chunks\n",
      "[Document(page_content='STOCK PURCHASE AGREEMENT\\nBY AND AMONG\\n[BUYER],\\n[TARGET COMPANY],\\nTHE SELLERS LISTED ON SCHEDULE I HERETO\\nAND\\nTHE SELLERS’ REPRESENTATIVE NAMED HEREIN\\nDated as of [●]', metadata={'source': '/home/alex/Building-RAG-based-LLM-Applications-for-Contract-Advisor/data/Raptor Contract.docx'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document\n",
    "\n",
    "# Load the content of the Word document\n",
    "document_path = \"/home/alex/Building-RAG-based-LLM-Applications-for-Contract-Advisor/data/Raptor Contract.docx\"\n",
    "document = Document(document_path)\n",
    "document_content = \"\\n\".join(paragraph.text for paragraph in document.paragraphs)\n",
    "\n",
    "# Text splitter\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Chunk the document\n",
    "chunks = text_splitter.create_documents(\n",
    "    texts=[document_content],\n",
    "    metadatas=[{\"source\": document_path}]\n",
    ")\n",
    "\n",
    "# Display the first chunk for verification\n",
    "print(f\"{len(chunks)} chunks\")\n",
    "print(chunks[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embed the chunk data\n",
    "Now that we've created small chunks from our sections, we need a way to identify the most relevant ones for a given query. A very effective and quick method is to embed our data using a pretrained model and use the same model to embed the query. We can then compute the distance between all of the chunk embeddings and our query embedding to determine the top-k chunks. There are many different pretrained models to choose from to embed our data but the most popular ones can be discovered through HuggingFace's Massive Text Embedding Benchmark (MTEB) leaderboard. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            self.embedding_model = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={\"device\": \"cuda\"},\n",
    "                encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100})\n",
    "\n",
    "    def __call__(self, chunks):\n",
    "        # Extract text from chunks\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "        # Embed the chunks in batches\n",
    "        batch_size = 8  # Adjust the batch size based on your system's capabilities\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch_texts)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        # Attach embeddings to each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk[\"embeddings\"] = embeddings[i]\n",
    "\n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-92ee429b2f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Embed chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"thenlper/gte-base\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m embedded_chunks = chunks.map_batches(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mEmbedChunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfn_constructor_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membedding_model_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map_batches'"
     ]
    }
   ],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
